task_lm: roberta-large
is_mask_lm: null
compute_zscore: true
incorrect_coeff: 180.0
correct_coeff: 200.0
dataset: sst-2
dataset_seed: 0
base_path: ./data
num_shots: 16
policy_lm: distilgpt2
hidden_size: 2048
logit_bias: 0.0
fluent: false
fluent_top_k: 20
max_decoding_length: 5
eos_token_id: null
prompt_length: 2
prompt_train_batch_size: 16
prompt_infer_batch_size: 1
source_str: <|endoftext|>
sql_loss_impl: v2_v2r_v3_v3r
training_mode: sql-onpolicy
mix_strategy: null
target_update_method: polyak
target_update_steps: null
target_learning_rate: 0.001
reward_shaping: true
reward_shaping_old_min: 0.0
reward_shaping_old_max: 1.0
reward_shaping_new_min: 0.0
reward_shaping_new_max: 5.0
top_k: 256
top_p: 1.0
num_beams: 1
train_batch_size: 16
train_shuffle: false
train_drop_last: true
num_train_epochs: 1
max_train_steps: 12000
do_eval: true
eval_batch_size: 16
eval_steps: 10
df_steps: 200
do_save: true
save_dir: ./outputs
save_steps: 100000
learning_rate: 5.0e-05
gradient_clip: true
gradient_clip_norm: 5.0
checkpoint_path: null
random_seed: null
report_to_wandb: true
project_name: rl-prompt
run_name: null
